"""
æ¨¡å‹æä¾›è€…å®ç°
åŒ…å«OpenAIé€‚é…å™¨å’Œæ¨¡å‹æä¾›è€…æ³¨å†Œè¡¨
"""

import os
from typing import Dict, List, Optional, Any, AsyncGenerator
import logging
import asyncio
import aiohttp
import json

from contracts.model_provider import (
    IModelProvider, ModelConfig, ModelResponse
)
from core.errors import APIError, ErrorCode


class OpenAIProvider(IModelProvider):
    """OpenAIæ¨¡å‹æä¾›è€…å®ç°"""
    
    def __init__(self, api_key: Optional[str] = None, logger: Optional[logging.Logger] = None, config_manager=None):
        # ä¼˜å…ˆä½¿ç”¨ä¼ å…¥çš„api_keyï¼Œç„¶åæ˜¯ç¯å¢ƒå˜é‡
        self.api_key = api_key or os.getenv("OPENAI_API_KEY")
        self.logger = logger or logging.getLogger(__name__)
        self.config_manager = config_manager
        
        # æ”¯æŒè‡ªå®šä¹‰APIç«¯ç‚¹
        self.base_url = os.getenv("OPENAI_API_BASE", "https://api.openai.com/v1")
        
        # é¢å¤–çš„é…ç½®é¡¹
        self.organization = os.getenv("OPENAI_ORG_ID")
        self.project = os.getenv("OPENAI_PROJECT_ID")
        self.timeout = self._parse_timeout(os.getenv("OPENAI_TIMEOUT", "30"))
        
        # æ¨¡å‹é…ç½®
        self.default_model = os.getenv("DEFAULT_MODEL", "qwen3")
        self.default_max_tokens = int(os.getenv("MAX_TOKENS", "2000"))
        self.default_temperature = float(os.getenv("TEMPERATURE", "0.7"))
        
        # Context7é…ç½®
        self.context7_enabled = os.getenv("CONTEXT7_ENABLED", "false").lower() == "true"
        
        # æ—¥å¿—é…ç½®
        self.log_config = self._load_log_config()
        
        # è®°å½•é…ç½®çŠ¶æ€
        if self.api_key:
            self.logger.info(f"OpenAI Provideré…ç½®æˆåŠŸï¼Œä½¿ç”¨æ¨¡å‹: {self.default_model}")
            if self.context7_enabled:
                self.logger.info("Context7é›†æˆå·²å¯ç”¨")
            if self.log_config["openai_request_logging"]:
                self.logger.info(f"ğŸ” OpenAIè¯·æ±‚æ—¥å¿—å·²å¯ç”¨ï¼Œçº§åˆ«: {self.log_config['openai_log_level']}")
        else:
            self.logger.warning("OpenAI APIå¯†é’¥æœªè®¾ç½®")
    
    def _load_log_config(self) -> Dict[str, Any]:
        """åŠ è½½æ—¥å¿—é…ç½®"""
        default_log_config = {
            "openai_request_logging": True,
            "openai_log_level": "INFO",
            "log_request_details": True,
            "log_response_details": True,
            "log_token_usage": True,
            "log_estimated_cost": True
        }
        
        if self.config_manager:
            try:
                return {
                    "openai_request_logging": self.config_manager.get_config_value("logging.openai_request_logging", True),
                    "openai_log_level": self.config_manager.get_config_value("logging.openai_log_level", "INFO"),
                    "log_request_details": self.config_manager.get_config_value("logging.log_request_details", True),
                    "log_response_details": self.config_manager.get_config_value("logging.log_response_details", True),
                    "log_token_usage": self.config_manager.get_config_value("logging.log_token_usage", True),
                    "log_estimated_cost": self.config_manager.get_config_value("logging.log_estimated_cost", True)
                }
            except Exception:
                return default_log_config
        else:
            # ä»ç¯å¢ƒå˜é‡è·å–é…ç½®
            return {
                "openai_request_logging": os.getenv("OPENAI_REQUEST_LOGGING", "true").lower() == "true",
                "openai_log_level": os.getenv("OPENAI_LOG_LEVEL", "INFO").upper(),
                "log_request_details": os.getenv("LOG_REQUEST_DETAILS", "true").lower() == "true",
                "log_response_details": os.getenv("LOG_RESPONSE_DETAILS", "true").lower() == "true",
                "log_token_usage": os.getenv("LOG_TOKEN_USAGE", "true").lower() == "true",
                "log_estimated_cost": os.getenv("LOG_ESTIMATED_COST", "true").lower() == "true"
            }
            
    def _parse_timeout(self, timeout_str: Optional[str]) -> float:
        """è§£æè¶…æ—¶é…ç½®"""
        if not timeout_str:
            return 30.0
        try:
            return float(timeout_str)
        except ValueError:
            return 30.0
    
    async def initialize(self, config: Dict[str, Any]) -> bool:
        """åˆå§‹åŒ–æä¾›è€…"""
        try:
            if not self.api_key:
                self.logger.error("OpenAI APIå¯†é’¥æœªè®¾ç½®")
                return False
            
            self.logger.info("OpenAI Provideråˆå§‹åŒ–æˆåŠŸ")
            return True
            
        except Exception as e:
            self.logger.error(f"OpenAI Provideråˆå§‹åŒ–å¤±è´¥: {e}")
            return False
    
    async def generate_response(
        self,
        messages: List[Dict[str, str]],
        config: ModelConfig
    ) -> ModelResponse:
        """ç”ŸæˆAIå“åº”"""
        try:
            # éªŒè¯è¾“å…¥
            if not messages:
                raise APIError("æ¶ˆæ¯åˆ—è¡¨ä¸èƒ½ä¸ºç©º", ErrorCode.API_REQUEST_INVALID)
            
            if not self.api_key:
                raise APIError("OpenAI APIå¯†é’¥æœªè®¾ç½®", ErrorCode.API_KEY_MISSING)
                
            # æ„å»ºAPIè¯·æ±‚å¤´
            headers = {
                "Content-Type": "application/json",
                "Authorization": f"Bearer {self.api_key}"
            }
            
            # æ·»åŠ ç»„ç»‡IDï¼ˆå¦‚æœæœ‰ï¼‰
            if self.organization:
                headers["OpenAI-Organization"] = self.organization
                
            # æ·»åŠ é¡¹ç›®IDï¼ˆå¦‚æœæœ‰ï¼‰
            if self.project:
                headers["OpenAI-Project"] = self.project
            
            endpoint = f"{self.base_url}/chat/completions"
            
            # ä½¿ç”¨ç¯å¢ƒå˜é‡ä¸­çš„é»˜è®¤å€¼ï¼Œå¦‚æœconfigä¸­æ²¡æœ‰æŒ‡å®š
            model_name = config.model_name or self.default_model
            max_tokens = config.max_tokens if config.max_tokens > 0 else self.default_max_tokens
            temperature = config.temperature if config.temperature >= 0 else self.default_temperature
            
            payload = {
                "model": model_name,
                "messages": messages,
                "temperature": temperature,
                "max_tokens": max_tokens
            }
            
            # æ ¹æ®é…ç½®æ§åˆ¶æ—¥å¿—è®°å½•
            if not self.log_config["openai_request_logging"]:
                # å¦‚æœç¦ç”¨OpenAIæ—¥å¿—ï¼Œåªè®°å½•åŸºæœ¬ä¿¡æ¯
                self.logger.debug(f"å‘é€OpenAIè¯·æ±‚: {model_name}")
            else:
                log_level = self.log_config["openai_log_level"]
                log_method = getattr(self.logger, log_level.lower(), self.logger.info)
                
                # åŸºæœ¬è¯·æ±‚ä¿¡æ¯
                log_method(f"ğŸš€ OpenAI APIè¯·æ±‚å¼€å§‹")
                
                if self.log_config["log_request_details"]:
                    log_method(f"ğŸ“ ç«¯ç‚¹: {endpoint}")
                    log_method(f"ğŸ¤– æ¨¡å‹: {model_name}")
                    log_method(f"ğŸŒ¡ï¸ æ¸©åº¦: {temperature}")
                    log_method(f"ğŸ“ æœ€å¤§tokens: {max_tokens}")
                    log_method(f"ğŸ’¬ æ¶ˆæ¯æ•°é‡: {len(messages)}")
                
                # è®°å½•æ¶ˆæ¯å†…å®¹ï¼ˆä»…åœ¨debugçº§åˆ«ï¼Œé¿å…æ•æ„Ÿä¿¡æ¯æ³„éœ²ï¼‰
                if self.logger.isEnabledFor(logging.DEBUG) and self.log_config["log_request_details"]:
                    self.logger.debug("ğŸ“‹ è¯·æ±‚æ¶ˆæ¯è¯¦æƒ…:")
                    for i, msg in enumerate(messages):
                        role = msg.get("role", "unknown")
                        content = msg.get("content", "")[:100] + "..." if len(msg.get("content", "")) > 100 else msg.get("content", "")
                        self.logger.debug(f"  {i+1}. [{role}]: {content}")
                
                # è®°å½•å®Œæ•´çš„è¯·æ±‚payloadï¼ˆdebugçº§åˆ«ï¼‰
                if self.logger.isEnabledFor(logging.DEBUG) and self.log_config["log_request_details"]:
                    payload_str = json.dumps(payload, indent=2, ensure_ascii=False)
                    self.logger.debug(f"ğŸ“¦ å®Œæ•´è¯·æ±‚payload:\n{payload_str}")
            
            # å‘é€APIè¯·æ±‚ï¼Œä½¿ç”¨é…ç½®çš„è¶…æ—¶æ—¶é—´
            import time
            start_time = time.time()
            
            timeout = aiohttp.ClientTimeout(total=self.timeout)
            async with aiohttp.ClientSession(timeout=timeout) as session:
                async with session.post(endpoint, headers=headers, json=payload) as resp:
                    end_time = time.time()
                    request_duration = end_time - start_time
                    
                    # æ ¹æ®é…ç½®è®°å½•è¯·æ±‚æ€§èƒ½å’ŒçŠ¶æ€
                    if self.log_config["openai_request_logging"]:
                        log_level = self.log_config["openai_log_level"]
                        log_method = getattr(self.logger, log_level.lower(), self.logger.info)
                        
                        log_method(f"â±ï¸ è¯·æ±‚è€—æ—¶: {request_duration:.2f}ç§’")
                        log_method(f"ğŸ“Š å“åº”çŠ¶æ€: {resp.status}")
                    
                    if resp.status != 200:
                        error_text = await resp.text()
                        self.logger.error(f"âŒ OpenAI APIé”™è¯¯ {resp.status}: {error_text}")
                        if self.log_config["openai_request_logging"] and self.log_config["log_request_details"]:
                            self.logger.error(f"ğŸ” è¯·æ±‚å¤´ (æ— æ•æ„Ÿä¿¡æ¯): {dict((k, v) for k, v in headers.items() if k != 'Authorization')}")
                        raise APIError(
                            f"OpenAI APIè°ƒç”¨å¤±è´¥: HTTP {resp.status}, {error_text}", 
                            ErrorCode.API_REQUEST_FAILED
                        )
                    
                    # è§£æå“åº”
                    result = await resp.json()
                    
                    # æ ¹æ®é…ç½®è®°å½•å“åº”ä¿¡æ¯
                    if self.log_config["openai_request_logging"]:
                        log_level = self.log_config["openai_log_level"]
                        log_method = getattr(self.logger, log_level.lower(), self.logger.info)
                        log_method("âœ… OpenAI APIå“åº”æˆåŠŸ")
                        
                        if self.logger.isEnabledFor(logging.DEBUG) and self.log_config["log_response_details"]:
                            response_str = json.dumps(result, indent=2, ensure_ascii=False)
                            self.logger.debug(f"ğŸ“¥ å®Œæ•´å“åº”:\n{response_str}")
                    
            # ä»å“åº”ä¸­æå–å†…å®¹
            response_content = result.get("choices", [{}])[0].get("message", {}).get("content", "")
            finish_reason = result.get("choices", [{}])[0].get("finish_reason", "unknown")
            
            # è·å–tokenä½¿ç”¨æƒ…å†µ
            usage = result.get("usage", {})
            total_tokens = usage.get("total_tokens", 0)
            prompt_tokens = usage.get("prompt_tokens", 0)
            completion_tokens = usage.get("completion_tokens", 0)
            
            # æ ¹æ®é…ç½®è®°å½•å“åº”è§£æä¿¡æ¯
            if self.log_config["openai_request_logging"]:
                log_level = self.log_config["openai_log_level"]
                log_method = getattr(self.logger, log_level.lower(), self.logger.info)
                
                if self.log_config["log_response_details"]:
                    log_method(f"ğŸ¯ å“åº”è§£æå®Œæˆ")
                    log_method(f"ğŸ å®ŒæˆåŸå› : {finish_reason}")
                    log_method(f"ğŸ“ å“åº”é•¿åº¦: {len(response_content)}å­—ç¬¦")
                
                # Tokenä½¿ç”¨æƒ…å†µæ—¥å¿—
                if self.log_config["log_token_usage"]:
                    log_method(f"ğŸ“Š Tokenä½¿ç”¨æƒ…å†µ:")
                    log_method(f"  â€¢ è¾“å…¥tokens: {prompt_tokens}")
                    log_method(f"  â€¢ è¾“å‡ºtokens: {completion_tokens}")
                    log_method(f"  â€¢ æ€»è®¡tokens: {total_tokens}")
                
                # è®°å½•å“åº”å†…å®¹é¢„è§ˆï¼ˆä»…åœ¨debugçº§åˆ«ï¼‰
                if self.logger.isEnabledFor(logging.DEBUG) and self.log_config["log_response_details"]:
                    content_preview = response_content[:200] + "..." if len(response_content) > 200 else response_content
                    self.logger.debug(f"ğŸ’¬ å“åº”å†…å®¹é¢„è§ˆ: {content_preview}")
                
                # è®¡ç®—ä¼°ç®—æˆæœ¬ï¼ˆåŸºäºtokenä½¿ç”¨ï¼‰
                if self.log_config["log_estimated_cost"]:
                    model_limits = self.get_model_limits(model_name)
                    if model_limits and "cost_per_1k_tokens" in model_limits:
                        estimated_cost = (total_tokens / 1000) * model_limits["cost_per_1k_tokens"]
                        log_method(f"ğŸ’° ä¼°ç®—æˆæœ¬: ${estimated_cost:.6f} USD")
            
            # åˆ›å»ºå“åº”å¯¹è±¡
            response = ModelResponse(
                content=response_content,
                usage_tokens=total_tokens,
                model=model_name,
                finish_reason=finish_reason,
                metadata={
                    "provider": "openai",
                    "temperature": temperature,
                    "max_tokens": max_tokens,
                    "prompt_tokens": prompt_tokens,
                    "completion_tokens": completion_tokens,
                    "context7_enabled": self.context7_enabled,
                    "request_duration": request_duration,
                    "endpoint": endpoint
                }
            )
            
            # æœ€ç»ˆå®Œæˆæ—¥å¿—
            if self.log_config["openai_request_logging"]:
                log_level = self.log_config["openai_log_level"]
                log_method = getattr(self.logger, log_level.lower(), self.logger.info)
                log_method(f"ğŸ‰ OpenAI APIè°ƒç”¨å®Œæˆï¼Œæ€»tokens: {total_tokens}")
            else:
                self.logger.debug(f"OpenAIè¯·æ±‚å®Œæˆï¼Œtokens: {total_tokens}")
            
            return response
            
        except aiohttp.ClientError as e:
            self.logger.error(f"ğŸŒ ç½‘ç»œè¯·æ±‚å¤±è´¥: {e}")
            self.logger.error(f"ğŸ” ç½‘ç»œé”™è¯¯è¯¦æƒ…:")
            self.logger.error(f"  â€¢ é”™è¯¯ç±»å‹: {type(e).__name__}")
            self.logger.error(f"  â€¢ é”™è¯¯æ¶ˆæ¯: {str(e)}")
            self.logger.error(f"  â€¢ ç«¯ç‚¹: {endpoint}")
            self.logger.error(f"  â€¢ è¶…æ—¶è®¾ç½®: {self.timeout}ç§’")
            
            return ModelResponse(
                content=f"æŠ±æ­‰ï¼Œç½‘ç»œè¿æ¥å‡ºç°é—®é¢˜: {str(e)}",
                usage_tokens=0,
                model=config.model_name or self.default_model,
                finish_reason="error",
                metadata={
                    "error": str(e), 
                    "error_type": "network",
                    "error_class": type(e).__name__,
                    "endpoint": endpoint
                }
            )
        except APIError as e:
            # APIError å·²ç»åœ¨ä¸Šé¢å¤„ç†è¿‡äº†ï¼Œè¿™é‡Œæ˜¯ä¸ºäº†é¿å…è¢«ä¸‹é¢çš„é€šç”¨å¼‚å¸¸æ•è·
            self.logger.error(f"ğŸš¨ APIé”™è¯¯: {e}")
            raise e
        except Exception as e:
            self.logger.error(f"ğŸ’¥ æœªé¢„æœŸçš„é”™è¯¯: {e}")
            self.logger.error(f"ğŸ” é”™è¯¯è¯¦æƒ…:")
            self.logger.error(f"  â€¢ é”™è¯¯ç±»å‹: {type(e).__name__}")
            self.logger.error(f"  â€¢ é”™è¯¯æ¶ˆæ¯: {str(e)}")
            self.logger.error(f"  â€¢ æ¨¡å‹: {config.model_name or self.default_model}")
            
            # åœ¨debugæ¨¡å¼ä¸‹è®°å½•å¼‚å¸¸å †æ ˆ
            if self.logger.isEnabledFor(logging.DEBUG):
                import traceback
                self.logger.debug(f"ğŸ“‹ å¼‚å¸¸å †æ ˆ:\n{traceback.format_exc()}")
            
            # è¿”å›å‹å¥½çš„é”™è¯¯å“åº”ï¼Œè€Œä¸æ˜¯ç›´æ¥æŠ›å‡ºå¼‚å¸¸
            return ModelResponse(
                content=f"æŠ±æ­‰ï¼ŒAIæœåŠ¡é‡åˆ°äº†é—®é¢˜: {str(e)}",
                usage_tokens=0,
                model=config.model_name or self.default_model,
                finish_reason="error",
                metadata={
                    "error": str(e),
                    "error_type": "general",
                    "error_class": type(e).__name__
                }
            )
    
    async def generate_stream_response(
        self,
        messages: List[Dict[str, str]],
        config: ModelConfig
    ) -> AsyncGenerator[str, None]:
        """ç”Ÿæˆæµå¼å“åº”"""
        try:
            # æ¨¡æ‹Ÿæµå¼å“åº”
            response_text = "è¿™æ˜¯ä¸€ä¸ªæ¨¡æ‹Ÿçš„æµå¼å“åº”ï¼Œå±•ç¤ºå¦‚ä½•åˆ†å—è¿”å›å†…å®¹ã€‚"
            words = response_text.split()
            
            for word in words:
                yield word + " "
                await asyncio.sleep(0.1)  # æ¨¡æ‹Ÿç½‘ç»œå»¶è¿Ÿ
            
        except Exception as e:
            self.logger.error(f"æµå¼å“åº”å¤±è´¥: {e}")
            raise APIError(f"OpenAIæµå¼APIè°ƒç”¨å¤±è´¥: {str(e)}", ErrorCode.API_REQUEST_FAILED)
    
    async def validate_config(self, config: ModelConfig) -> bool:
        """éªŒè¯æ¨¡å‹é…ç½®"""
        try:
            # æ£€æŸ¥æ¨¡å‹åç§°
            supported_models = ["qwen3", "gpt-4", "gpt-4-turbo"]
            if config.model_name not in supported_models:
                return False
            
            # æ£€æŸ¥å‚æ•°èŒƒå›´
            if not (0 <= config.temperature <= 2):
                return False
            
            if config.max_tokens <= 0:
                return False
            
            return True
            
        except Exception as e:
            self.logger.error(f"é…ç½®éªŒè¯å¤±è´¥: {e}")
            return False
    
    def get_supported_models(self) -> List[str]:
        """è·å–æ”¯æŒçš„æ¨¡å‹åˆ—è¡¨"""
        return [
            "qwen3",
            "gpt-4",
            "gpt-4-turbo",
            "gpt-4o"
        ]
    
    def get_provider_name(self) -> str:
        """è·å–æä¾›è€…åç§°"""
        return "openai"
    
    def get_model_limits(self, model: str) -> Dict[str, Any]:
        """è·å–æ¨¡å‹é™åˆ¶ä¿¡æ¯"""
        limits = {
            "qwen3": {
                "max_tokens": 4096,
                "context_window": 16384,
                "cost_per_1k_tokens": 0.002
            },
            "gpt-4": {
                "max_tokens": 8192,
                "context_window": 8192,
                "cost_per_1k_tokens": 0.03
            },
            "gpt-4-turbo": {
                "max_tokens": 4096,
                "context_window": 128000,
                "cost_per_1k_tokens": 0.01
            }
        }
        
        return limits.get(model, {})
    
    async def test_connection(self) -> bool:
        """æµ‹è¯•APIè¿æ¥"""
        try:
            # è¿™é‡Œåº”è¯¥è°ƒç”¨OpenAI APIçš„å¥åº·æ£€æŸ¥ç«¯ç‚¹
            # ç›®å‰è¿”å›Trueè¡¨ç¤ºè¿æ¥æ­£å¸¸
            return True
            
        except Exception as e:
            self.logger.error(f"è¿æ¥æµ‹è¯•å¤±è´¥: {e}")
            return False


class ModelProviderRegistry:
    """æ¨¡å‹æä¾›è€…æ³¨å†Œè¡¨"""
    
    def __init__(self, logger: Optional[logging.Logger] = None):
        self.logger = logger or logging.getLogger(__name__)
        self._providers: Dict[str, IModelProvider] = {}
        self._default_provider: Optional[str] = None
    
    def register_provider(self, name: str, provider: IModelProvider) -> bool:
        """æ³¨å†Œæ¨¡å‹æä¾›è€…"""
        try:
            if name in self._providers:
                self.logger.warning(f"æä¾›è€… {name} å·²å­˜åœ¨ï¼Œå°†è¢«è¦†ç›–")
            
            self._providers[name] = provider
            
            # è®¾ç½®ç¬¬ä¸€ä¸ªæ³¨å†Œçš„æä¾›è€…ä¸ºé»˜è®¤æä¾›è€…
            if not self._default_provider:
                self._default_provider = name
            
            self.logger.info(f"æä¾›è€… {name} æ³¨å†ŒæˆåŠŸ")
            return True
            
        except Exception as e:
            self.logger.error(f"æ³¨å†Œæä¾›è€…å¤±è´¥ {name}: {e}")
            return False
    
    def get_provider(self, name: Optional[str] = None) -> Optional[IModelProvider]:
        """è·å–æ¨¡å‹æä¾›è€…"""
        try:
            provider_name = name or self._default_provider
            
            if not provider_name:
                return None
            
            return self._providers.get(provider_name)
            
        except Exception as e:
            self.logger.error(f"è·å–æä¾›è€…å¤±è´¥ {name}: {e}")
            return None
    
    def list_providers(self) -> List[str]:
        """åˆ—å‡ºæ‰€æœ‰æ³¨å†Œçš„æä¾›è€…"""
        return list(self._providers.keys())
    
    def set_default_provider(self, name: str) -> bool:
        """è®¾ç½®é»˜è®¤æä¾›è€…"""
        if name in self._providers:
            self._default_provider = name
            self.logger.info(f"é»˜è®¤æä¾›è€…è®¾ç½®ä¸º: {name}")
            return True
        
        self.logger.error(f"æä¾›è€… {name} ä¸å­˜åœ¨")
        return False
    
    def get_default_provider(self) -> Optional[str]:
        """è·å–é»˜è®¤æä¾›è€…åç§°"""
        return self._default_provider
    
    async def initialize_all_providers(self, configs: Dict[str, Dict[str, Any]]) -> Dict[str, bool]:
        """åˆå§‹åŒ–æ‰€æœ‰æä¾›è€…"""
        results = {}
        
        for provider_name, provider in self._providers.items():
            config = configs.get(provider_name, {})
            
            try:
                if hasattr(provider, 'initialize'):
                    success = await provider.initialize(config)
                else:
                    success = True  # å¦‚æœæ²¡æœ‰initializeæ–¹æ³•ï¼Œè®¤ä¸ºæˆåŠŸ
                
                results[provider_name] = success
                
                if success:
                    self.logger.info(f"æä¾›è€… {provider_name} åˆå§‹åŒ–æˆåŠŸ")
                else:
                    self.logger.error(f"æä¾›è€… {provider_name} åˆå§‹åŒ–å¤±è´¥")
                    
            except Exception as e:
                self.logger.error(f"æä¾›è€… {provider_name} åˆå§‹åŒ–å¼‚å¸¸: {e}")
                results[provider_name] = False
        
        return results
    
    def unregister_provider(self, name: str) -> bool:
        """æ³¨é”€æä¾›è€…"""
        if name in self._providers:
            del self._providers[name]
            
            # å¦‚æœåˆ é™¤çš„æ˜¯é»˜è®¤æä¾›è€…ï¼Œé‡æ–°è®¾ç½®é»˜è®¤æä¾›è€…
            if self._default_provider == name:
                self._default_provider = next(iter(self._providers.keys())) if self._providers else None
            
            self.logger.info(f"æä¾›è€… {name} å·²æ³¨é”€")
            return True
        
        return False
